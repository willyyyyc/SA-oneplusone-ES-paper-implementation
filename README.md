# Reproducing the Results in: *Design of a Surrogate Model Assisted (1 + 1)-ES by Arash Kayhani and Dirk V. Arnold*
## Goal
As part of my NSERC research assistant placement, I was assigned this project to work on alongside my main tasks. The objective was to replicate the results from the professor’s paper to gain a thorough understanding of the foundational topics, as much of his current research builds upon the work presented in this paper.

## Background
Evolution Strategies are derivative free methods for optimization of non-linear or non-convex continuous optimization problems. It is a type of evolutionary algorithm, which are algorithms inspired by biological evolution. New candidate solutions are sampled according to a multivariate normal distribution in $\mathbb{R}^n$. In the case of (1 + 1)-ES, an offspring candidate solution $\mathbf{y}$ is generated by adding a standard normally distributed random vector $\mathbf{z}$ multiplied by a step size $\sigma$ to the current best candidate solution $\mathbf{x}$, also known as the parent.

$$\mathbf{y} = \mathbf{x} + \sigma \mathbf{z} \quad \mathbf{x}, \mathbf{z} \in \mathbb{R}^n \$$

Evolutionary optimization is often used for black-box optimization, which is the optimization of an objective function for which no analytical description is provided. In other words, there exists a mapping $\mathbf{x} \mapsto f(\mathbf{x})$, $\mathbb{R}^n \to \mathbb{R} \$, but this relationship is unknown. Evaluating the objective function is possible but is considered to be time-consuming or computationally expensive. Optimization strategies that do not require computing the derivative are therefore useful.

## Brief Paper Summary
Surrogate models are used in evolutionary algorithms to replace expensive function calls. They are cheaper, but usually inaccurate. The estimates are based on information gained in past iterations. The trade-off between computational savings versus poor steps due to inaccurate assessment of candidate solutions (because some evaluations are done using the model function instead of the objective black-box function) is studied in this paper, and the paper explores a “step size adaptation mechanism”.

Most of the information relevant to this project is found in Section 4 of the paper: Step Size Adaptation and Experiments.

“Natural implementation of preselection in the (1 + 1)-ES”:
- Obtain an estimate $f_e(\mathbf{y})$ of the objective function value using the surrogate model.
- If $f_e(\mathbf{y}) > f(\mathbf{x})$, i.e., surrogate model suggests that offspring candidate solution is inferior to the parent, then $\mathbf{y}$ is discarded. Proceed to next iteration.
- Otherwise, compute $f(\mathbf{y})$ at cost of one objective function call.
- If $f(\mathbf{y}) < f(\mathbf{x})$, then replace $\mathbf{x}$ with $\mathbf{y}$. Offspring solution is truly superior than the parent.

### Algorithm:
![Screenshot of the algorithm described in the paper.](/images/algorithm.png)

Constants $c_1$, $c_2$, and $c_3$ were determined to be 0.05, 0.2, and 0.6, respectively. Constant $D = \sqrt{1 + n}$. $\sigma$ is set initially to 1. The Gaussian process is used to generate surrogate models, and the squared exponential kernel is used (this I also implemented). $\theta$ (length scale parameter used in GP) is set to $8 \sigma \sqrt{n}$.

The training set is the 40 most recently evaluated points, and SA(1 + 1)-ES is only used after the 40th iteration. Prior to that, (1 + 1)-ES is used. The algorithm is the same as above, except it does not use the surrogate model and the step size either increments by $e^{0.8/D}$ or decrements by $e^{-0.2/D}$.

## Results:
101 tests were performed for each of the following objective functions (benchmark functions each with minimum at 0):
- $f(\mathbf{x}) = (\mathbf{x}^T\mathbf{x})^{\alpha/2})$, $\alpha = 1$  (Linear Sphere)
- $f(\mathbf{x}) = (\mathbf{x}^T\mathbf{x})^{\alpha/2})$, $\alpha = 2$  (Quadratic Sphere)
- $f(\mathbf{x}) = (\mathbf{x}^T\mathbf{x})^{\alpha/2})$, $\alpha = 3$  (Cubic Sphere)
- $f(\mathbf{x}) = \sum\limits_{i=1}^{n} (\sum\limits_{j=1}^{n} x_j)^2$ (Schwefel's Problem 1.2)
- $f(\mathbf{x}) = \sum\limits_{i=1}^{n-1}(\beta(x_{i+1} - x_i^2)^2 + (1 - x_i)^2)$, $\beta = 1$  (Quartic Function, Rosenbrock function when $\beta = 100$)

For each test, (1 + 1)-ES and SA(1 + 1)-ES were performed, and the total objective function calls were recorded. The speedup is the result of dividing the median number of objective function calls without model assistance by the median number of objective function calls with model assistance. The results from the paper, as shown in Table 1:

![Screenshot of the results from the paper.](/images/table-results.png)

And the results after my implementation:

![Screenshot of the results from my implementation.](/images/results.png)

## References
Kayhani, Arash, and Dirk V. Arnold. "Design of a surrogate model assisted (1+ 1)-ES." Parallel Problem Solving from Nature–PPSN XV: 15th International Conference, Coimbra, Portugal, September 8–12, 2018, Proceedings, Part I 15. Springer International Publishing, 2018. 

Paper in repository as 'paper.pdf'.

